{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFiWa55GT43K"
      },
      "source": [
        "# Reinforcement Learning: DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRf6ehkPX7Ji"
      },
      "source": [
        "- As before, please keep the names of the layer consistent with what is requested in model.py. Otherwise the test functions will not work\n",
        "\n",
        "- You will need to fill in the model.py, DQN.py, replay_buffer.py, and env_wrapper.py.\n",
        "\n",
        "To avoid pain with installation and model training, please use Colab for this project. DO NOT use Windows for this project. If you use Mac, there might be some potential test case failures you need to address with extra effort."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxCJ-NCrX-7p"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGaodQcus5uT",
        "outputId": "fa01cdd3-8c9c-4840-8b0c-e51679f61130"
      },
      "outputs": [],
      "source": [
        "# # mount it if you use Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnj_C4sBs_e0",
        "outputId": "350ec41e-3364-48e7-9b5d-8b51b65426fa"
      },
      "outputs": [],
      "source": [
        "# # TODO: change the dir to your folder\n",
        "# %cd /content/drive/MyDrive/YOUR_FOLDER_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VsidnRMqWOP"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.12.10' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ_RQITuqWOQ",
        "outputId": "399498ac-00e5-4276-c038-d6fb6712ab7a"
      },
      "outputs": [],
      "source": [
        "!pip install numpy torch wandb swig matplotlib termcolor\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBqeo1FFqWOQ"
      },
      "outputs": [],
      "source": [
        "import rl_test\n",
        "from utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR-fqedLqWOR"
      },
      "source": [
        "## Introduction to the Enviroment (5 points)\n",
        "We will be training a DQN agent to play the game of CarRacing. The agent will be trained to play the game using the pixels of the game as an input. The reward structure is as follows for each frame:\n",
        "- -0.1 for each frame\n",
        "- +1000/N where N is the number of tiles visited by the car in the episode\n",
        "\n",
        "The overall goal of this game is to design a agent that is able to play the game with a average test score of above 600. In discrete mode the actions can take 5 actions,\n",
        "- 0: Do Nothing\n",
        "- 1: Turn Left\n",
        "- 2: Turn Right\n",
        "- 3: Accelerate\n",
        "- 4: Brake\n",
        "\n",
        "First let us visualize the game and understand the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9Vyiz8xqWOR"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "env = gym.make('CarRacing-v3', continuous=False, render_mode='rgb_array')\n",
        "env.np_random = np.random.RandomState(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YWVWLqBnqWOS",
        "outputId": "777343d3-c837-45c1-abd1-e7414c1dfec7"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "frames = []\n",
        "s, _ = env.reset()\n",
        "\n",
        "while True:\n",
        "    a = env.action_space.sample()\n",
        "    s, r, terminated, truncated, _ = env.step(a)\n",
        "    frames.append(s)\n",
        "    if terminated or truncated:\n",
        "        break\n",
        "\n",
        "anim = animate(frames)\n",
        "HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2poBbTjqWOS"
      },
      "source": [
        "So a couple things we can note:\n",
        "- at the beginning of the game, we have 50 frames of the game slowly zooming into the car, we should ignore this period, ie no-op during this period.\n",
        "- there is a black bar at the bottom of the screen, we should crop this out of the observation.\n",
        "\n",
        "In addition, another thing to note is that the current frame doesn't give much information about the velocity and acceleration of the car, and that the car does not move much for each frame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6SgDVDpYXq-"
      },
      "source": [
        "### Environment Wrapper (5 points)\n",
        "As a result, you will need to complete `EnvWrapper` in `env_wrapper.py`. You can find more information in the docstring for the wrapper, however the main idea is that it is a wrapper to the environment that does the following:\n",
        "- skips the first 50 frames of the game\n",
        "- crops out the black bar and reshapes the observation to a 84x84 image, as well as turning the resulting image to grayscale\n",
        "- performs the actions for `skip_frames` frames\n",
        "- stacks the last `num_frames` frames together to give the agent some information about the velocity and acceleration of the car.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPyDPDUfqWOS",
        "outputId": "5b4af29a-8498-42e0-dd61-153f8bc71aa1"
      },
      "outputs": [],
      "source": [
        "from env_wrapper import EnvWrapper\n",
        "\n",
        "rl_test.test_wrapper(EnvWrapper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzG1IaSBZ35i"
      },
      "source": [
        "## DQN (25 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8xmz0mIqWOT"
      },
      "source": [
        "### CNN Model (5 points)\n",
        "Now we are ready to build the model. Our architecture of the CNN model is the one proposed by Mnih et al in \"Human-level control through deep reinforcement learning\". Specifically this consists of the following layers:\n",
        "- A convolutional layer with 32 filters of size 8x8 with stride 4 and relu activation\n",
        "- A convolutional layer with 64 filters of size 4x4 with stride 2 and relu activation\n",
        "- A convolutional layer with 64 filters of size 3x3 with stride 1 and relu activation\n",
        "- A fully connected layer with 512 units and relu activation\n",
        "- A fully connected layer with the number of outputs of the environment\n",
        "\n",
        "Please implement this model `Nature_Paper_Conv` in `model.py` as well as the helper\n",
        "`MLP` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC9fQSN_qWOT",
        "outputId": "b089d68d-69e2-462c-a2b4-49596cb27d4a"
      },
      "outputs": [],
      "source": [
        "import model\n",
        "rl_test.test_model_DQN(model.Nature_Paper_Conv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjF66WaiqWOT"
      },
      "source": [
        "Now we are ready to implement the DQN algorithm.\n",
        "\n",
        "![title](DQN.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zufn7A18ZDdj"
      },
      "source": [
        "### Replay Buffer (5 points)\n",
        "First start by implementing the DQN replay buffer `ReplayBufferDQN` in `replay_buffer.py`. This buffer will store the transitions of the agent and sample them for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5Ljv-fEqWOT",
        "outputId": "61b58179-3041-482c-9daa-0cef7d2339c6"
      },
      "outputs": [],
      "source": [
        "from replay_buffer import ReplayBufferDQN\n",
        "\n",
        "rl_test.test_DQN_replay_buffer(ReplayBufferDQN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoWER36sqWOU"
      },
      "source": [
        "\n",
        "### DQN (15 points)\n",
        "Now implement the `_optimize_model` and `_sample_action` functions in `DQN` in `DQN.py`. The `_optimize_model` function will sample a batch of transitions from the replay buffer and update the model. The `_sample_action` function will sample an action from the model given the current state. Train the model over 200 episdoes, validating every 50 episodes for 30 episodes, before testing the model for 50 episodes at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtU-UHgrHK93"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-OSv-xKHN42",
        "outputId": "e31824cd-fce2-45f6-d9d1-80feb9e30179"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "CzfSd_8xRU3H",
        "outputId": "16ff0196-4e0f-4f82-f1da-e57a2b24b7e3"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=\"racing-car-dqn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o931m_ZSqWOU",
        "outputId": "c309a9e4-43c2-4082-c1d2-8373f1f32025"
      },
      "outputs": [],
      "source": [
        "import DQN\n",
        "import utils\n",
        "import torch\n",
        "\n",
        "# if running with Colab T4 with wandb, it takes 60-70 mins\n",
        "# if running with Colab T4 w/o wandb, it takes ~30 mins\n",
        "# your test mean reward idealy should be larger than 400\n",
        "trainerDQN = DQN.DQN(EnvWrapper(env),\n",
        "                model.Nature_Paper_Conv,\n",
        "                lr = 0.00025,\n",
        "                gamma = 0.95,\n",
        "                buffer_size = 100000,\n",
        "                batch_size = 32,\n",
        "                loss_fn = \"mse_loss\",\n",
        "                use_wandb = False,\n",
        "                device = 'cuda',\n",
        "                seed = 42,\n",
        "                epsilon_scheduler = utils.exponential_decay(1, 700, 0.1),\n",
        "                save_path = utils.get_save_path(\"DQN\",\"./runs/\"))\n",
        "\n",
        "trainerDQN.train(200, 50, 30, 50, 50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bQ8ry-nqWOU"
      },
      "source": [
        "Please include a plot of the training and validation rewards over the episodes in the report. Anwer the question in no more than 2 sentences: Does the loss matter in DQN? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RbqfX1NbgQ0"
      },
      "source": [
        "Your answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1T06UJfe4Di"
      },
      "source": [
        "#### Animation\n",
        "\n",
        "We can also draw a animation of the car in one game, the code is provided below. The animation is just for your reference to get a feeling of how good your model is. No need to submit in your submission. We will grade based on your plots of training and validation above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "id": "fAluENIMqWOU",
        "outputId": "caec7294-b524-4a0c-826d-b392b2a00d58"
      },
      "outputs": [],
      "source": [
        "eval_env = gym.make('CarRacing-v3', continuous=True, render_mode='rgb_array')\n",
        "eval_env = EnvWrapper(eval_env)\n",
        "\n",
        "total_rewards, frames = trainerDQN.play_episode(0, True, 42)\n",
        "anim = animate(frames)\n",
        "HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJbdHcJbqWOV"
      },
      "source": [
        "## Double DQN (10 points)\n",
        "In the original paper, where the algorithim is shown above, the estimated target Q value was computed using the current Q network's weights. However, this can lead to overestimation of the Q values. To mitigate this, we can use the target network to compute the target Q value. This is known as Double DQN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJhbIpwFYvdz"
      },
      "source": [
        "### Hard updating Target Network (5 points)\n",
        "Original implementations for this involved hard updates, where the model weights were copied to the target network every C steps. This is known as hard updating. This was what was used in the Nature Paper by Mnih et al 2015 \"Human-level control through deep reinforcement learning\"\n",
        "\n",
        "Please implement this by implementing the `_optimize_model` and `_update_model` classes in `HardUpdateDQN` in `DQN.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNNRaAaHqWOV",
        "outputId": "ddff65a6-54bc-46fd-d515-0d5e7383da6c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import utils\n",
        "import DQN\n",
        "import model\n",
        "from env_wrapper import EnvWrapper\n",
        "\n",
        "trainerHardUpdateDQN = DQN.HardUpdateDQN(EnvWrapper(env),\n",
        "                model.Nature_Paper_Conv,\n",
        "                update_freq = 100,\n",
        "                lr = 0.00025,\n",
        "                gamma = 0.95,\n",
        "                buffer_size = 100000,\n",
        "                batch_size = 32,\n",
        "                loss_fn = \"mse_loss\",\n",
        "                use_wandb = False,\n",
        "                device = 'cuda',\n",
        "                seed = 42,\n",
        "                epsilon_scheduler = utils.exponential_decay(1, 1000, 0.1),\n",
        "                save_path = utils.get_save_path(\"DoubleDQN_HardUpdates/\",\"./runs/\"))\n",
        "\n",
        "trainerHardUpdateDQN.train(200, 50, 30, 50, 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKFEJEUKbS96"
      },
      "source": [
        "#### Animation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "id": "LLeJaVEGqWOV",
        "outputId": "c360e0f0-d080-4d0a-90ed-83102936b320"
      },
      "outputs": [],
      "source": [
        "total_rewards, frames = trainerHardUpdateDQN.play_episode(0, True, 42)\n",
        "anim = animate(frames)\n",
        "HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a27j9PeqWOV"
      },
      "source": [
        "### Soft Updates (5 points)\n",
        "A more recent improvement is to use soft updates, also known as Polyak averaging, where the target network is updated with a small fraction of the current model weights every step. In other words:\n",
        "$$\\theta_{target} = \\tau \\theta_{model} + (1-\\tau) \\theta_{target}$$\n",
        "for some $\\tau << 1$\n",
        "Please implement this by implementing the `_update_model` class in `SoftUpdateDQN` in `DQN.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npnRjBbmqWOV",
        "outputId": "cc8ef1f6-7fa9-408b-c915-e6c0a5912a01"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import utils\n",
        "import DQN\n",
        "import model\n",
        "from env_wrapper import EnvWrapper\n",
        "\n",
        "traineSoftUpdateDQN = DQN.SoftUpdateDQN(EnvWrapper(env),\n",
        "                model.Nature_Paper_Conv,\n",
        "                tau = 0.01,\n",
        "                update_freq = 1,\n",
        "                lr = 0.00025,\n",
        "                gamma = 0.95,\n",
        "                buffer_size = 100000,   # You can take smaller buffer size if you encounter RAM explosion\n",
        "                batch_size = 32,\n",
        "                loss_fn = \"mse_loss\",\n",
        "                use_wandb = False,\n",
        "                device = 'cuda',\n",
        "                seed = 42,\n",
        "                epsilon_scheduler = utils.exponential_decay(1, 1000,0.1),\n",
        "                save_path = utils.get_save_path(\"DoubleDQN_SoftUpdates\",\"./runs/\"))\n",
        "\n",
        "traineSoftUpdateDQN.train(200, 50, 30, 50, 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiB-95eLbZYR"
      },
      "source": [
        "#### Animation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEo53SzGqWOV"
      },
      "outputs": [],
      "source": [
        "total_rewards, frames = traineSoftUpdateDQN.play_episode(0, True, 42)\n",
        "anim = animate(frames)\n",
        "HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5bwvFkQqWOV"
      },
      "source": [
        "## Questions (10 points)\n",
        "- Which method performed better? (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITWggs72Zb-B"
      },
      "source": [
        "Your answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acuvEtzvZZPc"
      },
      "source": [
        "- If we modify the $\\tau$ for soft updates **or** the $C$ for the hard updates, how does this affect the performance of the model, come up with a intuition for this, then experimentally verify this.\n",
        " (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-AunIpzZi9C"
      },
      "source": [
        "Your answer:"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
