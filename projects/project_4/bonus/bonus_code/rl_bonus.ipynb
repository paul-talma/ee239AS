{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reinforcement Learning Bonus: DDPG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- As before, please do not modify the folder architecture and do not rename some of the files.\n",
        "\n",
        "- You will need to fill in the model.py and DDPG.py to solve the DoublePendulum environment.\n",
        "- Because this is a bonus, there will be no test cases.\n",
        "- This entire part will be worth 5 points of extra credit for project 4, and will be due on the same day as project 4, so June 6th.\n",
        "\n",
        "To avoid pain with installation and model training, we strongly recommend you to use Colab for this project. DO NOT use Windows for this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SiaejGWWY2y",
        "outputId": "40b4dcde-3a29-4cee-a102-b3d47e352d59"
      },
      "outputs": [],
      "source": [
        "# mount it if you use Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCo_z0hJWcdZ",
        "outputId": "49491091-1af8-4efb-80d1-23a72d57ce64"
      },
      "outputs": [],
      "source": [
        "# TODO: change the dir to your folder\n",
        "%cd /content/drive/MyDrive/YOUR_FOLDER_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H97ykZzFUKJE",
        "outputId": "75197dfe-1236-4fc9-959e-f45b7c2c4787"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%env MUJOCO_GL=egl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOu48qfnUKJj",
        "outputId": "6b083116-bc0a-4502-d947-0167ed8695c1"
      },
      "outputs": [],
      "source": [
        "!pip install numpy torch wandb swig gymnasium[mujoco] matplotlib termcolor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz99P9p3UKJo"
      },
      "outputs": [],
      "source": [
        "from utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAuMD1BaUKKA"
      },
      "source": [
        "## Introduction to the Enviroment\n",
        "We will be training a DDPG agent to solve the DoublePendulum environment. The DoublePendulum environment is a classic control problem where the goal is to balance a double pendulum on a cart.\n",
        "#### Action Space\n",
        "The agent can apply a force to the cart in the range of -1 to 1. This is a continuous action space.\n",
        "#### Observation Space\n",
        "The observation space is a 9 dimensional vector. The first 1 is the position of the cart, the next 4 are the cosines and sins of different angles of the double pendulum, and the next 3 are the velocities of the cart and the pendulum, and the final 1 is the constrain forces on the cart. You can find more information about these constraint forces [here](https://homes.cs.washington.edu/~todorov/papers/TodorovICRA14.pdf)\n",
        "#### Reward\n",
        "The reward can be decomposed into 3 parts. The first part is an alive bonus that pays +10 for every time step the second pendulum is upright. There are 2 penalty terms, one for the tip of the second pendulum moving too much, and another for the cart moving too fast.\n",
        "\n",
        "You can find more information about the environment [here](https://gymnasium.farama.org/environments/mujoco/inverted_double_pendulum/)\n",
        "\n",
        "First let us visualize the game and understand the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qwr3jfFuUKKD"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "env = gym.make(\"InvertedDoublePendulum-v5\")\n",
        "env.np_random = np.random.RandomState(42)\n",
        "\n",
        "eval_env = gym.make(\"InvertedDoublePendulum-v5\", render_mode=\"rgb_array\")\n",
        "eval_env.np_random = np.random.RandomState(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gzTEnD0yUKKD",
        "outputId": "2ad0ccd1-6212-4fe6-9ab5-1ef313dc6507"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "frames = []\n",
        "s, _ = eval_env.reset()\n",
        "\n",
        "while True:\n",
        "    a = eval_env.action_space.sample()\n",
        "    s, r, terminated, truncated, _ = eval_env.step(a)\n",
        "    frames.append(eval_env.render())\n",
        "    if terminated or truncated:\n",
        "        break\n",
        "\n",
        "anim = animate(frames)\n",
        "HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBfF0XtUUKKE"
      },
      "source": [
        "## Model (1 point)\n",
        "Because the inputs to the model is a 9 dimensional vector, we will use a MLP. Specifically we will follow the architecture in the DDPG paper. For DDPG we have both an Actor and a Critic. The Actor is responsible for selecting the action, and the Critic is responsible for evaluating the action.\n",
        "#### Actor\n",
        "The Actor is a 3 layer MLP:\n",
        "- Layer 1: 400 units, ReLU activation, Fan-in weight initialization, ie each weight is initialized with a uniform distribution in the range of -1/sqrt(fan_in) to 1/sqrt(fan_in)\n",
        "- Layer 2: 300 units, ReLU activation, Fan-in weight initialization, ie each weight is initialized with a uniform distribution in the range of -1/sqrt(fan_in) to 1/sqrt(fan_in)\n",
        "- Layer 3: 1 unit, tanh activation, intialized with uniform weights in the range of -0.003 to 0.003\n",
        "#### Critic\n",
        "The Critic is a 3 layer MLP:\n",
        "- Layer 1: 400 units, ReLU activation, Fan-in weight initialization, ie each weight is initialized with a uniform distribution in the range of -1/sqrt(fan_in) to 1/sqrt(fan_in)\n",
        "- Layer 2: 300 units, ReLU activation, Fan-in weight initialization, ie each weight is initialized with a uniform distribution in the range of -1/sqrt(fan_in) to 1/sqrt(fan_in). Input is the concatenation of the 400 dimension embedding from the state, and the action taken.\n",
        "- Layer 3: 1 unit, intialized with uniform weights in the range of -0.003 to 0.003"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEBMCPWTUKKI"
      },
      "outputs": [],
      "source": [
        "import model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRC-IC4TUKKI"
      },
      "source": [
        "## Exploration (1 point)\n",
        "Because DDPG is an off policy algorithm, we will use a noise process to encourage exploration. Specifically we will use the Ornstein-Uhlenbeck process. The Ornstein-Uhlenbeck process is a stochastic process that generates temporally correlated noise. The process is defined by the following stochastic differential equation:\n",
        "$$dx_t = \\theta(\\mu - x_t)dt + \\sigma dW_t$$\n",
        "Where $\\theta$ is the rate of mean reversion, $\\mu$ is the long run mean of the process, $\\sigma$ is the volatility of the process, and $W_t$ is a Wiener process. We can discretize this process to get the following:\n",
        "$$x_{t+1} = x_t + \\theta(\\mu - x_t)dt + \\sigma \\sqrt{dt}\\mathcal{N}(0,1)$$\n",
        "Where $N(0,1)$ is a sample from the standard normal distribution. We will asume that our steps are of unit length, so we can simplify this to:\n",
        "$$x_{t+1} = x_t + \\theta(\\mu - x_t) + \\sigma \\mathcal{N}(0,1)$$\n",
        "We will use $\\theta = 0.15$, $\\mu = 0$, and $\\sigma = 0.2$. We will add this to our action in the following way\n",
        "$$a_t = \\min(\\max(\\mu(s_t) + x_t, -1), 1)$$\n",
        "Where $a_t$ is the action taken by the agent, $\\mu(s_t)$ is the action selected by the actor, and $x_t$ is the noise generated by the Ornstein-Uhlenbeck process.\n",
        "Please implement the `OU_Noise` class in DDPG.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC7QM6nbUKKL"
      },
      "source": [
        "## DDPG (3 points total)\n",
        "We will be implementing the DDPG algorithm. The DDPG algorithm is a model free, off policy algorithm that combines the actor-critic architecture with the insights of DQN. The algorithm is as follows:\n",
        "![DDPG](DDPG.png)\n",
        "Fill in both of the TODO in the `DDPG` class in DDPG.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLFvkExsZYk0"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk2I4FKvdGvq"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=\"inverted-double-pendulum-ddpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uANt1728UKKT",
        "outputId": "86ee897d-5d06-4d39-d913-d5c31d5718ef"
      },
      "outputs": [],
      "source": [
        "import DDPG\n",
        "import utils\n",
        "t = DDPG.DDPG(env,\n",
        "            model.Actor,\n",
        "            model.Critic,\n",
        "            use_wandb=True,\n",
        "            save_path = utils.get_save_path(\"DDPG\",\"./runs/\"))\n",
        "\n",
        "t.train(10000,\n",
        "        100,\n",
        "        100,\n",
        "        1000,\n",
        "        100,\n",
        "        1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM6cg3oBUKKU"
      },
      "source": [
        "Like what we did for the DQN, we can also animate one episode of the agent in the DoublePendulum environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KhZW0YAiUKKU",
        "outputId": "97c3d473-c2d1-4847-9915-c917f452f916"
      },
      "outputs": [],
      "source": [
        "total_rewards, frames = t.play_episode(0,True,42,eval_env)\n",
        "anim = animate(frames)\n",
        "print(total_rewards)\n",
        "HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDX_-QaXUKKV"
      },
      "source": [
        "As we can see, the agent is able to balance the double pendulum and it eventually reaches the equilibrium. However this equilibrium is not a stable equilibrium, so lets see how this model performs with perturbations. To do this, we will perturbe the model every 49 steps with a large input of $\\pm 0.75$ N to the cart. We will see how the model performs with this perturbation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLVtiLK0UKKa",
        "outputId": "8ad2d502-b3b8-4b0f-8457-ec9b942b2b8c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "frames = []\n",
        "scores = 0\n",
        "(s, _), done, ret = eval_env.reset(seed = 42\n",
        "                                   ), False, 0\n",
        "t.actor.eval()\n",
        "S = []\n",
        "outputs = []\n",
        "# s, r, terminated, truncated, info = eval_env.step(3)\n",
        "i = 0\n",
        "with torch.no_grad():\n",
        "    while not done:\n",
        "        # if random.random() < 0.1:\n",
        "        #     action = random.randint(0,4)\n",
        "        # else:\n",
        "        frames.append(eval_env.render())\n",
        "        output = t.actor(torch.tensor(s).unsqueeze(0).to(\"cpu\").float())\n",
        "        i+=1\n",
        "        if i%50 == 49:\n",
        "            output += 0.75*(np.sign(torch.randn_like(output)))\n",
        "        s_prime, r, terminated, truncated, info = eval_env.step(output.cpu().numpy().squeeze(0))\n",
        "        s = s_prime\n",
        "        ret += r\n",
        "        done = terminated or truncated\n",
        "\n",
        "scores += ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xP39icFmUKKf",
        "outputId": "3e9ca543-d542-423c-c335-3b0e0ada2a1e"
      },
      "outputs": [],
      "source": [
        "anim = animate(frames)\n",
        "print(total_rewards)\n",
        "HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQhwG0wEUKKl"
      },
      "source": [
        "You should see that the model is able to recover from the perturbation and is able to balance the double pendulum."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
