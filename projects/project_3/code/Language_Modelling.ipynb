{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab Setup \n",
    "\n",
    "Please run the code below to mount drive if you are running on colab.\n",
    "\n",
    "Please ignore if you are running on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /content/drive/MyDrive/MiniGPT/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling and Transformers\n",
    "\n",
    "The project will consist of two broad parts. \n",
    "\n",
    "1. **Baseline Generative Language Model**: We will train a simple Bigram language model on the text data. We will use this model to generate a mini story. \n",
    "2. **Implementing Mini GPT**: We will implement a mini version of the GPT model layer by layer and attempt to train it on the text data. You will then load pretrained weights provided and generate a mini story. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some general instructions \n",
    "\n",
    "1. Please keep the name of layers consistent with what is requested in the `model.py` file for each layer, this helps us test in each function independently. \n",
    "2. Please check to see if the bias is to be set to false or true for all linear layers (it is mentioned in the doc string)\n",
    "3. As a general rule please read the docstring well, it contains information you will need to write the code. \n",
    "4. All configs are defined in `config.py` for the first part. While you are writing the code, do not change the values in the config file since we use them to test. Once you have passed all the tests please feel free to vary the parameter as you please.\n",
    "5. You will need to fill in `train.py` and run it to train the model. If you are running into memory issues please feel free to change the `batch_size` in the `config.py` file. If you are working on Colab please make sure to use the GPU runtime and feel free to copy over the training code to the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy torch tiktoken wandb einops # Install all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import BigramLanguageModel, SingleHeadAttention, MultiHeadAttention, FeedForwardLayer, LayerNorm, TransformerLayer, MiniGPT\n",
    "from config import BigramConfig, MiniGPTConfig\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not provided, download from https://drive.google.com/file/d/1g09qUM9WibdfQVgkj6IAj8K2S3SGwc91/view?usp=sharing\n",
    "path_to_bigram_tester = \"./pretrained_models/bigram_tester.pt\" # Load the bigram model with name bigram_tester.pt\n",
    "path_to_gpt_tester = \"./pretrained_models/minigpt_tester.pt\" # Load the gpt model with name minigpt_tester.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bigram Language Model (10 points)\n",
    "\n",
    "A bigram language model is a type of probabilistic language model that predicts a word given the previous word in the sequence. The model is trained on a text corpus and learns the probability of a word given the previous word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Bigram model (5 points)\n",
    "\n",
    "Please complete the `BigramLanguageModel` class in model.py. We will model a Bigram language model using a simple MLP with one hidden layer. The model will take in the previous word index and output the logits over the vocabulary for the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test implementation for Bigram Language Model\n",
    "from model import BigramLanguageModel, BigramConfig\n",
    "model = BigramLanguageModel(BigramConfig)\n",
    "tests.check_bigram(model, path_to_bigram_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Bigram Language Model (2.5 points)\n",
    "\n",
    "Complete the code in `train.py` to train the Bigram language model on the text data. Please provide plots for both the training and validation in the cell below.\n",
    "\n",
    "Some notes on the training process:\n",
    "\n",
    "1. You should be able to train the model slowly on your local machine.\n",
    "2. Training it on Colab will help with speed.\n",
    "3.  <span style=\"color:red\">To get full points for this section it is sufficient to show that the loss is decreasing over time</span>. You should see it saturate to a value close to around 5-6 but as long as you see it decreasing then saturating you should be good.\n",
    "4. Please log the loss curves either on wandb, tensorboard or any other logger of your choice and please attach them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver(model_name=\"bigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Valid Plots\n",
    "\n",
    "\n",
    "** Show the training and validation loss plots **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation (2.5 points)\n",
    "\n",
    "Complete the code in the `generate` method of the Bigram class and generate a mini story using the trained Bigram language model. The model will take in the previous word index and output the next word index.\n",
    "\n",
    "Start with the following seed sentence: \n",
    "    \n",
    "    `\"once upon a time\"`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Specify the path to your trained model\n",
    "model_path = \"/Users/paultalma/Documents/UCLA/Work/Classes/2024-2025/ee_239/projects/project_3/code/models/bigram/mini_model_checkpoint_500000.pt\"\n",
    "model = BigramLanguageModel(BigramConfig)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('mps'))[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text starting with: torch.Size([4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paultalma/Documents/UCLA/Work/Classes/2024-2025/ee_239/projects/project_3/code/model.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  context = torch.tensor(context, device=device)\n",
      "/Users/paultalma/Documents/UCLA/Work/Classes/2024-2025/ee_239/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, where he wanted to play friendly bunny Ben had never seen something unexpected happened when I want to the rock gasped. One day, \"I'm friends. He put him near a time. It looked happy that the attic got a picture.\n",
      "From that day Billy.ze.\" She had an done, no matter how to love. They all Mrs. But the jelly. His mommy wanted to play and said, she found him a bigan was the store. We can do when you cry and that jumped two friends. They all played with cats letters.\"\n",
      "But I'll fix fire to her to count him, a pick out her mom was a!\" Lily was very happy. Tom wanted town saw a moment and had honest and Lily if you!\" spun. The seal for lunch.\" The sun. Sam did not scared. Tim was a walk in the park with her a little girl named Lily. It was a group of the town, Lily just was too. brought lots of a cage\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "gen_sent = \"Once upon a time\"\n",
    "gen_tokens = torch.tensor(tokenizer.encode(gen_sent), device=device)\n",
    "print(\"Generating text starting with:\", gen_tokens.shape)\n",
    "model.eval()\n",
    "output = tokenizer.decode(model.generate(gen_tokens, max_new_tokens=200))\n",
    "print(gen_sent, end=\"\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation and Analysis\n",
    "\n",
    "Please answer the following questions. \n",
    "\n",
    "1. What can we say about the generated text in terms of grammar and coherence? \n",
    "\n",
    "The text is largely incoherent and ungrammatical. Syntax and semantics just aren't there.\n",
    "\n",
    "2. What are the limitations of the Bigram language model?\n",
    "\n",
    "The Bigram model can only encode one-step statistical dependencies between words. But linguistic structure depends on more complex longer-range dependencies (\"John borrowed Anna's book. He gave it back to her later\" — the grammatical gender of \"her\" depends on the content of the first sentence.)\n",
    "\n",
    "3. If the model is scaled with more parameters do you expect the bigram model to get substantially better? Why or why not?\n",
    "\n",
    "For the reasons given in part 2, I do not expect the model to get better with more parameters. More parameters would allow better modeling of one-step dependencies. But no matter how good your modeling of one-step dependencies is, you will not be able to recover even moderately interesting linguistic structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini GPT (90 points)\n",
    "\n",
    "We will implement a decoder style transformer model like we discussed in lecture, which is a scaled down version of the [GPT model](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf). \n",
    "\n",
    "All the model components follow directly from the original [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper. The only difference is we will use prenormalization and learnt positional embeddings instead of fixed ones.\n",
    "\n",
    "We will now implement each layer step by step checking if it is implemented correctly in the process. We will finally put together all our layers to get a fully fledged GPT model. \n",
    "\n",
    "<span style=\"color:red\">Later layers might depend on previous layers so please make sure to check the previous layers before moving on to the next one.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Head Causal Attention (20 points)\n",
    "\n",
    "We will first implement the single head causal attention layer. This layer is the same as the scaled dot product attention layer but with a causal mask to prevent the model from looking into the future.\n",
    "\n",
    "Recall that Each head has a Key, Query and Value Matrix and the scaled dot product attention is calculated as : \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\end{equation}\n",
    "\n",
    "where $d_k$ is the dimension of the key matrix.\n",
    "\n",
    "Figure below from the original paper shows how the layer is to be implemented.\n",
    "\n",
    "![image](./Images/Single_Head.png)\n",
    "\n",
    "Image credits: [Attention is All You Need Paper](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the `SingleHeadAttention` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SingleHeadAttention(MiniGPTConfig.embed_dim, MiniGPTConfig.embed_dim//4, MiniGPTConfig.embed_dim//4) # configs are set as such for testing do not modify\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "tests.check_singleheadattention(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention (10 points)\n",
    "\n",
    "Now that we have a single head working, we will now scale this across multiple heads, remember that with multihead attention we compute perform head number of parallel attention operations. We then concatenate the outputs of these parallel attention operations and project them back to the desired dimension using an output linear layer.\n",
    "\n",
    "Figure below from the original paper shows how the layer is to be implemented.\n",
    "\n",
    "![image](./Images/MultiHead.png)\n",
    "\n",
    "Image credits: [Attention is All You Need Paper](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the `MultiHeadAttention` class in `model.py` using the `SingleHeadAttention` class implemented earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiHeadAttention(MiniGPTConfig.embed_dim, MiniGPTConfig.num_heads)\n",
    "tests.check_multiheadattention(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Layer (5 points)\n",
    "\n",
    "As discussed in lecture, the attention layer is completely linear, in order to add some non-linearity we add a feed forward layer. The feed forward layer is a simple two layer MLP with a GeLU activation in between.\n",
    "\n",
    "Please complete the `FeedForwardLayer` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FeedForwardLayer(MiniGPTConfig.embed_dim)\n",
    "tests.check_feedforward(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm (10 points)\n",
    "\n",
    "We will now implement the layer normalization layer. Layernorm is used across the model to normalize the activations of the previous layer. Recall that the equation for layernorm is given as:\n",
    "\n",
    "\\begin{equation}\n",
    "\n",
    "\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\odot \\gamma + \\beta\n",
    "\n",
    "\\end{equation}\n",
    "\n",
    "With the learnable parameters $\\gamma$ and $\\beta$. \n",
    "\n",
    "Remember that unlike batchnorm we compute statistics across the feature dimension and not the batch dimension, hence we do not need to keep track of running averages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the `LayerNorm` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LayerNorm(MiniGPTConfig.embed_dim)\n",
    "tests.check_layernorm(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Layer (15 points)\n",
    "\n",
    "We have now implemented all the components of the transformer layer. We will now put it all together to create a transformer layer. The transformer layer consists of a multi head attention layer, a feed forward layer and two layer norm layers.\n",
    "\n",
    "Please use the following order for each component (Varies slightly from the original attention paper):\n",
    "1. LayerNorm\n",
    "2. MultiHeadAttention\n",
    "3. LayerNorm\n",
    "4. FeedForwardLayer\n",
    "\n",
    "Remember that the transformer layer also has residual connections around each sublayer.\n",
    "\n",
    "The below figure shows the structure of the transformer layer you are required to implement.\n",
    "\n",
    "![prenorm_transformer](./Images/Prenorm.png)\n",
    "\n",
    "Image Credit : [CogView](https://arxiv.org/pdf/2105.13290)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `TransformerLayer` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =  TransformerLayer(MiniGPTConfig.embed_dim, MiniGPTConfig.num_heads)\n",
    "tests.check_transformer(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together : MiniGPT (15 points)\n",
    "\n",
    "We are now ready to put all our layers together to build our own MiniGPT! \n",
    "\n",
    "The MiniGPT model consists of an embedding layer, a positional encoding layer and a stack of transformer layers. The output of the transformer layer is passed through a linear layer (called head) to get the final output logits. Note that in our implementation we will use [weight tying](https://arxiv.org/abs/1608.05859) between the embedding layer and the final linear layer. This allows us to save on parameters and also helps in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `MiniGPT` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = MiniGPTConfig(\n",
    "    to_log=False,\n",
    "    save_iterations=100_000\n",
    ")\n",
    "model = MiniGPT(config)\n",
    "tests.check_miniGPT(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt at training the model (5 points)\n",
    "\n",
    "We will now attempt to train the model on the text data. We will use the same text data as before. If needed, you can scale down the model parameters in the config file to a smaller value to make training feasible. \n",
    "\n",
    "Use the same training script we built for the Bigram model to train the MiniGPT model. If you implemented it correctly it should work just out of the box!\n",
    "\n",
    "**NOTE** : We will not be able to train the model to completion in this assignment. Unfortunately, without access to a relatively powerful GPU, training a large enough model to see good generation is not feasible. However, you should be able to see the loss decreasing over time. <span style=\"color:red\">To get full points for this section it is sufficient to show that the loss is decreasing over time</span>. You do not need to run this for more than 5000 iterations or 1 hour of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters: 3.32M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:03,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Train Loss: 10.805253982543945\n",
      "Average eval loss: 10.865549112753893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1194it [00:25, 58.42it/s]"
     ]
    }
   ],
   "source": [
    "solver(model_name=\"minigpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Valid Plots\n",
    "\n",
    "\n",
    "** Show the training and validation loss plots **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation (5 points)\n",
    "\n",
    "\n",
    "Perform generation with the MiniGPT model that you trained. After that, copy over the generation function you used for the Bigram model and generate a mini story using the same seed sentence. \n",
    "\n",
    "    `\"once upon a time\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Specify the path to your trained model\n",
    "model_path = None\n",
    "model = MiniGPT(MiniGPTConfig)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "gen_sent = \"Once upon a time\"\n",
    "gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
    "print(\"Generating text starting with:\", gen_tokens.shape)\n",
    "gen_tokens = gen_tokens.to(device)\n",
    "model.eval()\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please answer the following questions. \n",
    "\n",
    "1. What can we say about the generated text in terms of grammar and coherence? \n",
    "2. If the model is scaled with more parameters do you expect the GPT model to get substantially better? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling up the model (5 points)\n",
    "\n",
    "To show that scale indeed will help the model learn we have trained a scaled up version of the model you just implemented. We will load the weights of this model and generate a mini story using the same seed sentence. Note that if you have implemented the model correctly just scaling the parameters and adding a few bells and whistles to the training script will results in a model like the one we will load now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MiniGPT\n",
    "from config import MiniGPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_trained_model = \"pretrained_models/best_train_loss_checkpoint.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(path_to_trained_model, map_location=device) # remove map location if using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the configs for scaled model \n",
    "MiniGPTConfig.context_length = 512\n",
    "MiniGPTConfig.embed_dim = 256\n",
    "MiniGPTConfig.num_heads = 16\n",
    "MiniGPTConfig.num_layers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from checkpoint\n",
    "model = MiniGPT(MiniGPTConfig)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "gen_sent = \"Once upon a time\"\n",
    "gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
    "print(\"Generating text starting with:\", gen_tokens.shape)\n",
    "gen_tokens = gen_tokens.to(device)\n",
    "model.eval()\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (5 points)\n",
    "\n",
    "The following are some open ended questions that you can attempt if you have time. Feel free to propose your own as well if you have an interesting idea. \n",
    "\n",
    "1. The model we have implemented is a decoder only model. Can you implement the encoder part as well? This should not be too hard to do since most of the layers are already implemented.\n",
    "2. What are some improvements we can add to the training script to make training more efficient and faster? Can you concretely show that the improvements you made help in training the model better?\n",
    "3. Can you implement a beam search decoder to generate the text instead of greedy decoding? Does this help in generating better text?\n",
    "4. Can you further optimize the model architecture? For example, can you implement [Multi Query Attention](https://arxiv.org/abs/1911.02150) or [Grouped Query Attention](https://arxiv.org/pdf/2305.13245) to improve the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
