{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XV8xdKBCnmav"
   },
   "source": [
    "**We would like to acknowledge University of Michigan's EECS 498-007/598-005 on which we based the development of this project.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJZ8AefthL95"
   },
   "source": [
    "\n",
    "# Variational Autoencoder\n",
    "\n",
    "In this notebook, you will implement a variational autoencoder and a conditional variational autoencoder with slightly different architectures and apply them to the popular MNIST handwritten dataset. Recall from C147/C247, an autoencoder seeks to learn a latent representation of our training images by using unlabeled data and learning to reconstruct its inputs. The *variational autoencoder* extends this model by adding a probabilistic spin to the encoder and decoder, allowing us to sample from the learned distribution of the latent space to generate new images at inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtA1_PsYhs24"
   },
   "source": [
    "## Setup Code\n",
    "Before getting started, we need to run some boilerplate code to set up our environment. You'll need to rerun this setup code each time you start the notebook.\n",
    "\n",
    "First, run this cell that loads the autoreload extension. This allows us to edit .py source files and re-import them into the notebook for a seamless editing and debugging experience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1743924769728,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "OKXSEQjRh63r"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "USE_COLAB = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIXWSou6h_S6"
   },
   "source": [
    "### Google Colab Setup\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link and sign in to your Google account (the same account you used to store this notebook!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25214,
     "status": "ok",
     "timestamp": 1743924794933,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "evqEGDXRipC-",
    "outputId": "83b2003d-5404-452c-d47b-e7908f80dacc"
   },
   "outputs": [],
   "source": [
    "if USE_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2y_JbZTlDoai"
   },
   "source": [
    "Now recall the path in your Google Drive where you uploaded this notebook and fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:\n",
    "\n",
    "```\n",
    "['vae.ipynb', 'nndl2', 'vae.py']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2303,
     "status": "ok",
     "timestamp": 1743924797241,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "Z8OU2pRkivyc",
    "outputId": "2fed0b50-75d7-4161-9ea2-618a9f4db99b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if USE_COLAB:\n",
    "    # TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "    # Example: '239AS.2/project1/vae'\n",
    "    GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '239AS.2/project1/vae'\n",
    "    GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "    print(os.listdir(GOOGLE_DRIVE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJ7auXOMi4rw"
   },
   "source": [
    "Once you have successfully mounted your Google Drive and located the path to\n",
    "this assignment, run the following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n",
    "\n",
    "```\n",
    "Hello from vae.py!\n",
    "Hello from helper.py!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1743925025698,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "5zOOPYIUjCUO",
    "outputId": "10c13aa5-c491-40c4-d572-75db76d24b95"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if USE_COLAB:\n",
    "    sys.path.append(GOOGLE_DRIVE_PATH)\n",
    "\n",
    "from vae import hello_vae\n",
    "hello_vae()\n",
    "\n",
    "from nndl2.helper import hello_helper\n",
    "hello_helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuIiv2bhjFoC"
   },
   "source": [
    "Load several useful packages that are used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1380,
     "status": "ok",
     "timestamp": 1743924812398,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "sLdT7GSljI0f"
   },
   "outputs": [],
   "source": [
    "from nndl2.grad import rel_error\n",
    "from nndl2.utils import reset_seed\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# for plotting\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nqWhiLojS8M"
   },
   "source": [
    "We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1743924812429,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "RdQhVgi5jVQp",
    "outputId": "69b42295-5edb-4d4b-b161-351d643fbbf7"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Good to go!')\n",
    "else:\n",
    "    print('Please set GPU via the downward triangle in the top right corner.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcqRQILRjchz"
   },
   "source": [
    "## Load MNIST Dataset\n",
    "\n",
    "\n",
    "VAEs are notoriously finicky with hyperparameters, and also require many training epochs. In order to make this assignment approachable, we will be working on the MNIST dataset, which is 60,000 training and 10,000 test images. Each picture contains a centered image of white digit on black background (0 through 9). This was one of the first datasets used to train convolutional neural networks and it is fairly easy -- a standard CNN model can easily exceed 99% accuracy.\n",
    "\n",
    "To simplify our code here, we will use the PyTorch MNIST wrapper, which downloads and loads the MNIST dataset. See the [documentation](https://github.com/pytorch/vision/blob/master/torchvision/datasets/mnist.py) for more information about the interface. The default parameters will take 5,000 of the training examples and place them into a validation dataset. The data will be saved into a folder called `MNIST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2424,
     "status": "ok",
     "timestamp": 1743924825575,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "mExnwvTXjcF_",
    "outputId": "74879fdb-6a81-4242-8ea8-562b8a17f8a8"
   },
   "outputs": [],
   "source": [
    "if USE_COLAB:\n",
    "    %cd /content/drive/My\\ Drive/$GOOGLE_DRIVE_PATH_AFTER_MYDRIVE\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "mnist_train = dset.MNIST('./nndl2', train=True, download=True,\n",
    "                           transform=T.ToTensor())\n",
    "loader_train = DataLoader(mnist_train, batch_size=batch_size,\n",
    "                          shuffle=True, drop_last=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwDmYBjdhTrM"
   },
   "source": [
    "## Visualize dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2X_21cTwsox"
   },
   "source": [
    "It is always a good idea to look at examples from the dataset before working with it. Let's visualize the digits in the MNIST dataset. We have defined the function `show_images` in `helper.py` that we call to visualize the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 897
    },
    "executionInfo": {
     "elapsed": 4243,
     "status": "ok",
     "timestamp": 1743924834822,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "3JMbbxMkwrYg",
    "outputId": "bd2c0c36-c5af-4256-f4e6-ab6b38acdd31"
   },
   "outputs": [],
   "source": [
    "from nndl2.helper import show_images\n",
    "\n",
    "imgs = next(iter(loader_train))[0].view(batch_size, 784)\n",
    "show_images(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOdQ3r5diEwr"
   },
   "source": [
    "# Fully Connected VAE\n",
    "\n",
    "Our first VAE implementation will consist solely of fully connected layers. We'll take the `1 x 28 x 28` shape of our input and flatten the features to create an input dimension size of 784. In this section you'll define the Encoder and Decoder models in the VAE class of `vae.py` and implement the reparametrization trick, forward pass, and loss function to train your first VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqMjCTSMi0jX"
   },
   "source": [
    "## FC-VAE Encoder (4 points)\n",
    "\n",
    "Now lets start building our fully-connected VAE network. We'll start with the encoder, which will take our images as input (after flattening C,H,W to D shape) and pass them through a three Linear+ReLU layers. We'll use this hidden dimension representation to predict both the posterior mu and posterior log-variance using two separate linear layers (both shape (N,Z)).\n",
    "\n",
    "Note that we are calling this the 'logvar' layer because we'll use the log-variance (instead of variance or standard deviation) to stabilize training. This will specifically matter more when you compute reparametrization and the loss function later.\n",
    "\n",
    "*Define `hidden_dim=400`, `encoder`, `mu_layer`, and `logvar_layer` in the initialization of the VAE class in `vae.py`. Use nn.Sequential to define the encoder, and separate Linear layers for the mu and logvar layers. Architecture for the encoder is described below:*\n",
    "\n",
    "\n",
    " * `Flatten` (Hint: nn.Flatten)\n",
    " * Fully connected layer with input size `input_size` and output size `hidden_dim`\n",
    " * `ReLU`\n",
    " * Fully connected layer with input_size `hidden_dim` and output size `hidden_dim`\n",
    " * `ReLU`\n",
    " * Fully connected layer with input_size `hidden_dim` and output size `hidden_dim`\n",
    " * `ReLU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1743925582323,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "Wy0kdCXn-Mug",
    "outputId": "c2f75140-65a3-4d0c-ad4d-9b46c58f3713"
   },
   "outputs": [],
   "source": [
    "from vae import VAE\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "def test_encoder(model, input_size, hidden_dim, n_encoder_lin_layers):\n",
    "    '''\n",
    "    model: Model defined as above\n",
    "    input_size: dimensionality of input\n",
    "    hidden_dim: dimensionality of hidden state\n",
    "    n_layers: number of Linear layers\n",
    "    '''\n",
    "    expected_n_params = (input_size+1)*hidden_dim + \\\n",
    "        (n_encoder_lin_layers-1)*(hidden_dim+1)*hidden_dim\n",
    "    actual_n_params = count_params(model.encoder)\n",
    "    if actual_n_params == expected_n_params:\n",
    "        print('Correct number of parameters in model.encoder.')\n",
    "        return True\n",
    "    else:\n",
    "        print('Incorrect number of parameters in model.encoder.' \\\n",
    "          ' model.encoder does not include mu_layer and the logvar_layer.' \\\n",
    "          ' Check your achitecture.')\n",
    "        return False\n",
    "    return\n",
    "def test_mu_logvar(model, hidden_dim, latent_size):\n",
    "    '''\n",
    "    model: Model defined as above\n",
    "    input_size: dimensionality of input\n",
    "    hidden_dim: dimensionality of hidden state\n",
    "    n_layers: number of Linear layers\n",
    "    '''\n",
    "    if count_params(model.mu_layer) == (hidden_dim+1)*latent_size:\n",
    "        print('Correct number of parameters in model.mu_layer.')\n",
    "    else:\n",
    "        print('Incorrect number of parameters in model.mu_layer.')\n",
    "    if count_params(model.logvar_layer) == (hidden_dim+1)*latent_size:\n",
    "        print('Correct number of parameters in model.logvar_layer.')\n",
    "    else:\n",
    "        print('Incorrect number of parameters in model.logvar_layer.')\n",
    "    return\n",
    "test_encoder(VAE(345, 17), 345, 400, 3)\n",
    "test_mu_logvar(VAE(345, 17), 400, 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTuEAFrgkTyt"
   },
   "source": [
    "## FC-VAE Decoder (1 point)\n",
    "\n",
    "We'll now define the decoder, which will take the latent space representation and generate a reconstructed image. The architecture is as follows:\n",
    "\n",
    " * Fully connected layer with input size `latent_size` and output size `hidden_dim`\n",
    " * `ReLU`\n",
    " * Fully connected layer with input_size `hidden_dim` and output size `hidden_dim`\n",
    " * `ReLU`\n",
    " * Fully connected layer with input_size `hidden_dim` and output size `hidden_dim`\n",
    " * `ReLU`\n",
    " * Fully connected layer with input_size `hidden_dim` and output size `input_size`\n",
    " * `Sigmoid`\n",
    " * `Unflatten` (nn.Unflatten)\n",
    "\n",
    "*Define a `decoder` in the initialization of the VAE class in `vae.py`. Like the encoding step, use `nn.Sequential`*  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 111,
     "status": "ok",
     "timestamp": 1743925803897,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "ID6-RxL-DSZe",
    "outputId": "26fd8979-b94e-4988-a991-f341650b22ec"
   },
   "outputs": [],
   "source": [
    "from vae import VAE\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "def test_decoder(model, input_size, hidden_dim, latent_size, n_decoder_lin_layers):\n",
    "    '''\n",
    "    model: Model defined as above\n",
    "    input_size: dimensionality of input\n",
    "    hidden_dim: dimensionality of hidden state\n",
    "    latent_size: dimensionality of latent space\n",
    "    n_layers: number of Linear layers in model.decoder\n",
    "    '''\n",
    "    expected_n_params = (latent_size+1)*hidden_dim + \\\n",
    "        (n_decoder_lin_layers-2)*(hidden_dim+1)*hidden_dim + \\\n",
    "        (hidden_dim+1)*input_size\n",
    "    actual_n_params = count_params(model.decoder)\n",
    "    if actual_n_params == expected_n_params:\n",
    "        print('Correct number of parameters in model.decoder.')\n",
    "    else:\n",
    "        print('Incorrect number of parameters in model.decoder.')\n",
    "    return\n",
    "test_decoder(VAE(345, 17), 345, 400, 17, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFTb-35TiOZl"
   },
   "source": [
    "## Reparametrization (2 points)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdD23s3Vf70-"
   },
   "source": [
    "Now we'll apply a reparametrization trick in order to estimate the posterior $z$ during our forward pass, given the $\\mu$ and $\\sigma^2$ estimated by the encoder. A simple way to do this could be to simply generate a normal distribution centered at our  $\\mu$ and having a std corresponding to our $\\sigma^2$. However, we would have to backpropogate through this random sampling that is not differentiable. Instead, we sample initial random data $\\epsilon$ from a fixed distrubtion, and compute $z$ as a function of ($\\epsilon$, $\\sigma^2$, $\\mu$). Specifically:\n",
    "\n",
    "$z = \\mu + \\sigma\\epsilon$\n",
    "\n",
    "We can easily find the partial derivatives w.r.t $\\mu$ and $\\sigma^2$ and backpropagate through $z$. If $\\epsilon = \\mathcal{N} (0,1)$, then its easy to verify that the result of our forward pass calculation will be a distribution centered at $\\mu$ with variance $\\sigma^2$.\n",
    "\n",
    "Implement `reparametrization` in `vae.py` and verify your mean and std error are at or less than `1e-4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1743926422803,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "T236XnbhbVH4",
    "outputId": "d73d8722-f43e-4ba4-8a65-c6b3aa15229d"
   },
   "outputs": [],
   "source": [
    "reset_seed(0)\n",
    "from vae import reparametrize\n",
    "latent_size = 15\n",
    "size = (1, latent_size)\n",
    "mu = torch.zeros(size)\n",
    "logvar = torch.ones(size)\n",
    "\n",
    "z = reparametrize(mu, logvar)\n",
    "\n",
    "expected_mean = torch.FloatTensor([-0.4363])\n",
    "expected_std = torch.FloatTensor([1.6860])\n",
    "z_mean = torch.mean(z, dim=-1)\n",
    "z_std = torch.std(z, dim=-1)\n",
    "assert z.size() == size\n",
    "\n",
    "print('Mean Error', rel_error(z_mean, expected_mean))\n",
    "print('Std Error', rel_error(z_std, expected_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOfC7oDrkUhl"
   },
   "source": [
    "## FC-VAE Forward (1 point)\n",
    "\n",
    "Complete the VAE class by writing the forward pass. The forward pass should pass the input image through the encoder to calculate the estimation of mu and logvar, reparametrize to estimate the latent space z, and finally pass z into the decoder to generate an image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1743929329776,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "0DQzFuZ9GA7F",
    "outputId": "8692fcbf-f49a-47bb-e475-b84d804a8d19"
   },
   "outputs": [],
   "source": [
    "from vae import VAE\n",
    "def test_VAE_shapes():\n",
    "    all_shapes_correct = True\n",
    "    with torch.no_grad():\n",
    "        batch_size = 3\n",
    "        latent_size = 17\n",
    "        x_hat, mu, logvar = VAE(28*28, latent_size)(torch.ones(batch_size, 1, 28, 28))\n",
    "        if x_hat.shape != (batch_size, 1, 28, 28):\n",
    "            print(f'x_hat has incorrect shape. Expected (batch_size, 1, 28, 28) = ({batch_size}, 1, 28, 28).'\n",
    "                f' Got {tuple(x_hat.shape)}.')\n",
    "            all_shapes_correct = False\n",
    "        if mu.shape != (batch_size, latent_size):\n",
    "            print(f'mu has incorrect shape. Expected (batch_size, latent_size) = ({batch_size}, {latent_size}).'\n",
    "                f' Got {tuple(mu.shape)}.')\n",
    "            all_shapes_correct = False\n",
    "        if logvar.shape != (batch_size, latent_size):\n",
    "            print(f'logvar has incorrect shape. Expected (batch_size, latent_size) = ({batch_size}, {latent_size}).'\n",
    "                f' Got {tuple(logvar.shape)}.')\n",
    "            all_shapes_correct = False\n",
    "        if all_shapes_correct:\n",
    "            print('Shapes of x_hat, mu, and logvar are correct.')\n",
    "        if batch_size > 1 and x_hat.std(0).mean() == 0:\n",
    "            print('x_hat has no randomness.')\n",
    "    return\n",
    "test_VAE_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ0UXWyMi9Gu"
   },
   "source": [
    "## Loss Function (1 point)\n",
    "\n",
    "Before we're able to train our final model, we'll need to define our loss function. As seen below, the loss function for VAEs contains two terms: A reconstruction loss term (left) and KL divergence term (right).\n",
    "\n",
    "$-E_{Z~q_{\\phi}(z|x)}[log p_{\\theta}(x|z)] + D_{KL}(q_{\\phi}(z|x), p(z)))$\n",
    "\n",
    "Note that this is the negative of the variational lowerbound shown in lecture--this ensures that when we are minimizing this loss term, we're maximizing the variational lowerbound. The reconstruction loss term can be computed by simply using the binary cross entropy loss between the original input pixels and the output pixels of our decoder (Hint: `nn.functional.binary_cross_entropy`). The KL divergence term works to force the latent space distribution to be close to a prior distribution (we're using a standard normal gaussian as our prior).\n",
    "\n",
    "To help you out, we've derived an unvectorized form of the KL divergence term for you.\n",
    "Suppose that $q_\\phi(z|x)$ is a $Z$-dimensional diagonal Gaussian with mean $\\mu_{z|x}$ of shape $(Z,)$ and standard deviation $\\sigma_{z|x}$ of shape $(Z,)$, and that $p(z)$ is a $Z$-dimensional Gaussian with zero mean and unit variance. Then we can write the KL divergence term as:\n",
    "\n",
    "$D_{KL}(q_{\\phi}(z|x), p(z))) = -\\frac{1}{2} \\sum_{j=1}^{J} (1 + log(\\sigma_{z|x}^2)_{j} - (\\mu_{z|x})^2_{j} - (\\sigma_{z|x})^2_{j}$)\n",
    "\n",
    "It's up to you to implement a vectorized version of this loss that also operates on minibatches.\n",
    "You should average the loss across samples in the minibatch.\n",
    "\n",
    "Implement `loss_function` in `vae.py` and verify your implementation below. Your relative error should be less than or equal to `1e-5`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1743926557398,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "vF2ZUj2FjrFa",
    "outputId": "dbf757f0-ad5c-46a8-c269-0261c1488eb6"
   },
   "outputs": [],
   "source": [
    "from vae import loss_function\n",
    "size = (1,15)\n",
    "\n",
    "image_hat = torch.sigmoid(torch.FloatTensor([[2,5], [6,7]]).unsqueeze(0).unsqueeze(0))\n",
    "image = torch.sigmoid(torch.FloatTensor([[1,10], [9,3]]).unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "expected_out = torch.tensor(8.5079)\n",
    "mu, logvar = torch.ones(size), torch.zeros(size)\n",
    "out = loss_function(image_hat, image, mu, logvar)\n",
    "print('Loss error', rel_error(expected_out,out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wV8fbzenkAXm"
   },
   "source": [
    "\n",
    "## Train a model\n",
    "\n",
    "Now that we have our VAE defined and loss function ready, lets train our model! Our training script is provided  in `nndl2/helper.py`, and we have pre-defined an Adam optimizer, learning rate, and # of epochs for you to use.\n",
    "\n",
    "Training for 10 epochs should take ~2 minutes and your loss should be less than 120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 258941,
     "status": "ok",
     "timestamp": 1743927010197,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "rWaaacNHsfao",
    "outputId": "80ca0000-9c31-4fbc-9783-ab8d08e8a7ce"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "latent_size = 15\n",
    "from vae import VAE\n",
    "from nndl2.helper import train_vae\n",
    "input_size = 28*28\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cpu':\n",
    "    print(f'Warning: using device {device} may take longer.')\n",
    "vae_model = VAE(input_size, latent_size=latent_size)\n",
    "vae_model.to(device)\n",
    "for epoch in range(0, num_epochs):\n",
    "    train_vae(epoch, vae_model, loader_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JT6Ek-26jjJD"
   },
   "source": [
    "## Visualize results\n",
    "\n",
    "After training our VAE network, we're able to take advantage of its power to generate new training examples. This process simply involves the decoder: we intialize some random distribution for our latent spaces z, and generate new examples by passing these latent space into the decoder.\n",
    "\n",
    "Run the cell below to generate new images! You should be able to visually recognize many of the digits, although some may be a bit blurry or badly formed. Our next model will see improvement in these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1743927070673,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "RhhrsgrMTyTi",
    "outputId": "888e4d89-7a43-487f-f66b-8e22fbf60534"
   },
   "outputs": [],
   "source": [
    "device = next(vae_model.parameters()).device\n",
    "z = torch.randn(10, latent_size).to(device=device)\n",
    "import matplotlib.gridspec as gridspec\n",
    "vae_model.eval()\n",
    "samples = vae_model.decoder(z).data.cpu().numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 1))\n",
    "gspec = gridspec.GridSpec(1, 10)\n",
    "gspec.update(wspace=0.05, hspace=0.05)\n",
    "for i, sample in enumerate(samples):\n",
    "    ax = plt.subplot(gspec[i])\n",
    "    plt.axis('off')\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_aspect('equal')\n",
    "    plt.imshow(sample.reshape(28,28), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sx3HGSpXk1MY"
   },
   "source": [
    "## Latent Space Interpolation\n",
    "\n",
    "As a final visual test of our trained VAE model, we can perform interpolation in latent space. We generate random latent vectors $z_0$ and $z_1$, and linearly interplate between them; we run each interpolated vector through the trained generator to produce an image.\n",
    "\n",
    "Each row of the figure below interpolates between two random vectors. For the most part the model should exhibit smooth transitions along each row, demonstrating that the model has learned something nontrivial about the underlying spatial structure of the digits it is modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "executionInfo": {
     "elapsed": 3129,
     "status": "ok",
     "timestamp": 1743927097672,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "XZ_4XsFURmN1",
    "outputId": "bdfc04b4-1b5e-4778-c408-084418cfdbfd"
   },
   "outputs": [],
   "source": [
    "S = 12\n",
    "latent_size = 15\n",
    "device = next(vae_model.parameters()).device\n",
    "z0 = torch.randn(S, latent_size, device=device)\n",
    "z1 = torch.randn(S, latent_size, device=device)\n",
    "w = torch.linspace(0, 1, S, device=device).view(S, 1, 1)\n",
    "z = (w * z0 + (1 - w) * z1).transpose(0, 1).reshape(S * S, latent_size)\n",
    "x = vae_model.decoder(z)\n",
    "show_images(x.data.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzS_ufzEkhah"
   },
   "source": [
    "# Conditional FC-VAE\n",
    "\n",
    "The second model you'll develop will be very similar to the FC-VAE, but with a slight conditional twist to it. We'll use what we know about the labels of each MNIST image, and *condition* our latent space and image generation on the specific class. Instead of $q_{\\phi} (z|x)$ and $p_{\\phi}(x|z)$ we have $q_{\\phi} (z|x,c)$  and $p_{\\phi}(x|z, c)$\n",
    "\n",
    "This will allow us to do some powerful conditional generation at inference time. We can specifically choose to generate more 1s, 2s, 9s, etc. instead of simply generating new digits randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hle0JuhwklKc"
   },
   "source": [
    "## Define Network with class input (3 points)\n",
    "\n",
    "Our CVAE architecture will be the same as our FC-VAE architecture, except we'll now add a one-hot label vector to both the x input (in our case, the flattened image dimensions) and the z latent space.\n",
    "\n",
    "If our one-hot vector is called `c`, then `c[label] = 1` and `c = 0` elsewhere.\n",
    "\n",
    "For the `CVAE` class in `vae.py` use the same FC-VAE architecture implemented in the last network with the following modifications:\n",
    "\n",
    "1. Modify the first linear layer of your `encoder` to take in not only the flattened input image, but also the one-hot label vector `c`. The CVAE `encoder` should not have a `Flatten` layer.\n",
    "2. Modify the first layer of your `decoder` to project the latent space + one-hot vector to the `hidden_dim`\n",
    "3. Lastly, implement the `forward` pass to combine the flattened input image with the one-hot vectors (`torch.cat`) before passing them to the `encoder` and combine the latent space with the one-hot vectors (`torch.cat`) before passing them to the `decoder`. You should flatten the image before concatenation (e.g. with `torch.flatten` or `torch.reshape`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1743929227202,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "7cNQpAcQM2p3",
    "outputId": "815af883-3767-458f-c45d-85bd890e1580"
   },
   "outputs": [],
   "source": [
    "from vae import CVAE\n",
    "def test_CVAE_shapes():\n",
    "    all_shapes_correct = True\n",
    "    with torch.no_grad():\n",
    "        batch_size = 3\n",
    "        num_classes = 10\n",
    "        latent_size = 17\n",
    "        cls = nn.functional.one_hot(torch.tensor([3]*batch_size, dtype=torch.long), num_classes=num_classes)\n",
    "        x_hat, mu, logvar = CVAE(28*28, num_classes=num_classes,latent_size=latent_size)(\n",
    "            torch.ones(batch_size, 1, 28, 28), cls)\n",
    "        if x_hat.shape != (batch_size, 1, 28, 28):\n",
    "            print(f'x_hat has incorrect shape. Expected (batch_size, 1, 28, 28) = ({batch_size}, 1, 28, 28).'\n",
    "                f' Got {tuple(x_hat.shape)}.')\n",
    "            all_shapes_correct = False\n",
    "        if mu.shape != (batch_size, latent_size):\n",
    "            print(f'mu has incorrect shape. Expected (batch_size, latent_size) = ({batch_size}, {latent_size}).'\n",
    "                f' Got {tuple(mu.shape)}.')\n",
    "            all_shapes_correct = False\n",
    "        if logvar.shape != (batch_size, latent_size):\n",
    "            print(f'logvar has incorrect shape. Expected (batch_size, latent_size) = ({batch_size}, {latent_size}).'\n",
    "                f' Got {tuple(logvar.shape)}.')\n",
    "            all_shapes_correct = False\n",
    "        if all_shapes_correct:\n",
    "            print('Shapes of x_hat, mu, and logvar are correct.')\n",
    "        if batch_size > 1 and x_hat.std(0).mean() == 0:\n",
    "            print('x_hat has no randomness.')\n",
    "    return\n",
    "test_CVAE_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUzKyFI9kp8i"
   },
   "source": [
    "## Train model\n",
    "\n",
    "Using the same training script, let's now train our CVAE!\n",
    "\n",
    "Training for 10 epochs should take ~2 minutes and your loss should be less than 120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234127,
     "status": "ok",
     "timestamp": 1743927893374,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "N1dzKDUsunbD",
    "outputId": "b44ade83-d963-4027-8631-5020234f8898"
   },
   "outputs": [],
   "source": [
    "from vae import CVAE\n",
    "num_epochs = 10\n",
    "latent_size = 15\n",
    "from nndl2.helper import train_vae\n",
    "input_size = 28*28\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cpu':\n",
    "    print(f'Warning: using device {device} may take longer.')\n",
    "\n",
    "cvae = CVAE(input_size, latent_size=latent_size)\n",
    "cvae.to(device)\n",
    "for epoch in range(0, num_epochs):\n",
    "    train_vae(epoch, cvae, loader_train, cond=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMAyFBZTkr1Y"
   },
   "source": [
    "## Visualize Results\n",
    "\n",
    "We've trained our CVAE, now lets conditionally generate some new data! This time, we can specify the class we want to generate by adding our one hot matrix of class labels. We use `torch.eye` to create an identity matrix, gives effectively gives us one label for each digit. When you run the cell below, you should get one example per digit. Each digit should be reasonably distinguishable (it is ok to run this cell a few times to save your best results).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1743928126221,
     "user": {
      "displayName": "JOHANNES LEE",
      "userId": "15854150438463122195"
     },
     "user_tz": 420
    },
    "id": "GCfwpz0NALdZ",
    "outputId": "f6509f3b-8f96-4f12-adca-9bef5cabec31"
   },
   "outputs": [],
   "source": [
    "device = next(cvae.parameters()).device\n",
    "z = torch.randn(10, latent_size)\n",
    "c = torch.eye(10, 10) # [one hot labels for 0-9]\n",
    "import matplotlib.gridspec as gridspec\n",
    "z = torch.cat((z,c), dim=-1).to(device=device)\n",
    "cvae.eval()\n",
    "samples = cvae.decoder(z).data.cpu().numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 1))\n",
    "gspec = gridspec.GridSpec(1, 10)\n",
    "gspec.update(wspace=0.05, hspace=0.05)\n",
    "for i, sample in enumerate(samples):\n",
    "    ax = plt.subplot(gspec[i])\n",
    "    plt.axis('off')\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_aspect('equal')\n",
    "    plt.imshow(sample.reshape(28, 28), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gU0FJy7wN2k8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
